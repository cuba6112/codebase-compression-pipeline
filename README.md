# Codebase Compression Pipeline

A high-performance, scalable pipeline for compressing large codebases into optimized representations for Large Language Model (LLM) context windows. This tool enables LLMs to process significantly larger codebases by intelligently reducing token count while preserving semantic meaning and structural relationships.

## ğŸš€ Features

- **Multi-Language Support**: Python, JavaScript, TypeScript, Go, and Rust parsers with AST-based analysis
- **Intelligent Compression**: Multiple compression strategies (full, structural, signature, summary)
- **Streaming Architecture**: Memory-efficient processing of arbitrarily large codebases
- **Parallel Processing**: Work-stealing queue implementation for optimal CPU utilization
- **Advanced Caching**: Content-hash based incremental processing with LZ4 compression
- **Security Validation**: Built-in security scanning and input validation
- **Real-time Monitoring**: Performance tracking with bottleneck detection
- **Resilience Patterns**: Circuit breakers, retry logic, and fallback mechanisms

## ğŸ—ï¸ Architecture

The pipeline processes codebases through 8 distinct stages:

1. **File Parsing & Tokenization** - Language-specific AST parsing and metadata extraction
2. **Parallel Processing** - Work-stealing queue for optimal resource utilization  
3. **Streaming Compression** - Memory-efficient LZ4 compression with deduplication
4. **Incremental Caching** - Content-hash based change detection and caching
5. **Metadata Storage** - Columnar storage for efficient querying
6. **Query-Based Selection** - Selective compression based on filter criteria
7. **Output Formatting** - Multiple output formats (Markdown, JSON, XML)
8. **Performance Optimization** - Real-time monitoring and auto-tuning

## ğŸ“¦ Installation

### Dependencies

```bash
pip install lz4 mmh3 psutil tqdm
```

### Optional Dependencies

For enhanced functionality:
```bash
pip install redis numpy brotli zstandard  # For Redis caching and additional compression
npm install typescript @typescript-eslint/parser  # For TypeScript support
```

## ğŸ¯ Quick Start

### Command Line Usage

```bash
# Basic compression (current directory)
python codebase_compression_pipeline.py

# Compress specific directory
python codebase_compression_pipeline.py --path /path/to/project

# With custom format and strategy
python codebase_compression_pipeline.py --path /path/to/project --format json --strategy structural

# Enable profiling for performance analysis
python codebase_compression_pipeline.py --enable-profiling

# Process specific file types only
python codebase_compression_pipeline.py --extensions .py,.js,.ts
```

### Python API

```python
from pathlib import Path
from codebase_compression_pipeline import CodebaseCompressionPipeline

# Initialize pipeline
pipeline = CodebaseCompressionPipeline(
    cache_dir=Path('./cache'),
    output_dir=Path('./compressed_output'),
    num_workers=4
)

# Process codebase
output_files = pipeline.process_codebase(
    codebase_path=Path('./my_project'),
    output_format='markdown',
    compression_strategy='structural',
    query_filter={'language': 'python', 'min_complexity': 5.0}
)

# Cleanup
pipeline.cleanup()
```

## âš™ï¸ Configuration

### Compression Strategies

- **`full`**: Complete code with minimal compression
- **`structural`**: Preserve structure, compress implementation details  
- **`signature`**: Function/class signatures and interfaces only
- **`summary`**: High-level overview and key components

### Output Formats

- **`markdown`**: Human-readable format optimized for LLMs
- **`json`**: Structured data for programmatic processing
- **`xml`**: Hierarchical representation with metadata

### Pre-configured Settings

```python
from pipeline_configs import ConfigPresets

# For large codebases (>100k files)
config = ConfigPresets.large_codebase()

# For memory-constrained environments (<4GB RAM)
config = ConfigPresets.memory_constrained()

# For real-time processing
config = ConfigPresets.real_time()

# For maximum compression ratio
config = ConfigPresets.maximum_compression()
```

## ğŸ”§ Advanced Features

### Security Validation

Built-in security scanning for:
- Path traversal prevention
- Malicious code detection
- File type validation
- Content sanitization

### Performance Monitoring

Real-time metrics for:
- Processing throughput (files/sec, MB/sec)
- Memory usage and optimization
- Bottleneck detection
- Stage-level performance analysis

### Caching Options

- **Local**: File-based caching with LZ4 compression
- **Redis**: Distributed caching for multi-instance deployments
- **Hybrid**: Combines local and Redis for optimal performance

## ğŸ§ª Testing

Run the test suite:

```bash
# All tests
python run_tests.py

# With coverage
python run_tests.py --coverage

# Verbose output
python run_tests.py --verbose

# Fast tests only (skip performance tests)
python run_tests.py --fast

# Parallel execution
python run_tests.py --parallel 4
```

## ğŸ“Š Performance

Typical performance on modern hardware:
- **Processing Speed**: 500+ files/second
- **Memory Usage**: Constant O(1) regardless of codebase size
- **Compression Ratio**: 10:1 to 50:1 depending on strategy
- **Cache Hit Rate**: >90% for incremental processing

## ğŸ” Use Cases

- **Code Review**: Compress large PRs for LLM-assisted review
- **Documentation**: Generate concise codebase summaries
- **Analysis**: Extract patterns and dependencies across projects
- **Migration**: Understand legacy codebases for modernization
- **Education**: Create digestible code examples for learning

## ğŸ› ï¸ Project Structure

```
project_think/
â”œâ”€â”€ codebase_compression_pipeline.py    # Main pipeline implementation
â”œâ”€â”€ pipeline_configs.py                 # Configuration presets
â”œâ”€â”€ pipeline_monitoring.py              # Performance monitoring
â”œâ”€â”€ security_validation.py              # Security scanning
â”œâ”€â”€ parsers/                            # Language-specific parsers
â”‚   â”œâ”€â”€ python_parser.py
â”‚   â”œâ”€â”€ javascript_parser.py
â”‚   â””â”€â”€ base.py
â”œâ”€â”€ pipeline/                           # Pipeline stages
â”‚   â”œâ”€â”€ stages/
â”‚   â””â”€â”€ workers/
â”œâ”€â”€ tests/                              # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â””â”€â”€ requirements.txt                    # Dependencies
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LZ4**: High-speed compression library
- **MurMurHash3**: Fast hashing for content deduplication
- **psutil**: System resource monitoring
- **AST**: Abstract Syntax Tree parsing for semantic analysis

## ğŸ“ˆ Roadmap

- [ ] GraphQL query interface for selective compression
- [ ] Plugin system for custom language parsers
- [ ] Web interface for interactive exploration
- [ ] Integration with popular IDEs and editors
- [ ] Machine learning-based compression optimization